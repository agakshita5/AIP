{% extends "base.html" %}

{% block content %}
<div class="welcome-content">
    <h2>Explore Adversarial Attacks</h2>
    <p class="tagline">Generate, detect, and defend against adversarial perturbations in machine learning models</p>

    <div class="feature-grid">
        <a href="{{ url_for('generate') }}" class="feature-card">
            <i class="fas fa-code feature-icon"></i>
            <h3 class="feature-title">Generate Attacks</h3>
            <p class="feature-text">Create adversarial examples using FGSM to fool neural networks</p>
            <div class="feature-hover-anim"></div>
        </a>

        <a href="{{ url_for('detect') }}" class="feature-card">
            <i class="fas fa-search feature-icon"></i>
            <h3 class="feature-title">Detect Attacks</h3>
            <p class="feature-text">Identify potentially adversarial images using advanced analysis</p>
            <div class="feature-hover-anim"></div>
        </a>

        <a href="{{ url_for('defend') }}" class="feature-card">
            <i class="fas fa-shield-alt feature-icon"></i>
            <h3 class="feature-title">Defend Models</h3>
            <p class="feature-text">Apply defensive techniques to counter adversarial attacks</p>
            <div class="feature-hover-anim"></div>
        </a>
    </div>

    <div class="info-section">
        <h3>About Adversarial Attacks</h3>
        <p>Adversarial attacks are specially crafted inputs designed to fool machine learning models. 
        These perturbations are often imperceptible to humans but can cause models to make incorrect predictions.</p>
    </div>
</div>
{% endblock %}
